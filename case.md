# Historical development

## Introduction 

The journey that mankind has taken in developing computing is an incredible journey consisting of thousands of years, in which mankind has explored many different disciplines: science, engineering, mathematics and technology. The progression from mechanical calculating machines from the 1800's through to today's quantum and digital computers has represented a convergence of theoretical and practical discovery, scientific experimentation, and mathematical abstraction. The three documents presented explore these developments in light of the historical evolution of machines and systems, the scientific perspective about how physical, logical, and informational principles made the rise of computing possible, and the mathematical perspective on how abstract concepts were turned into functional devices and computational architectures.

This timeline begins with Charles Babbage, whose difference engines demonstrated for the first time that mathematical operations could be mechanised, introducing a revolutionary way of thinking about calculation. The study will lead you to Alan Turing's work on his theoretical model for computability and will offer you a theoretical model for creating algorithms or developing software applications that follow Turing's theoretical principles for the creation and execution of computer programs. The development of ENIAC was one of the first models of electronic computers, creating an explosion of advancement from the mechanical to electronic and from there, to modern computing technology based on Turing's theoretical principles. Additionally, the development of the transistor and integrated circuits has drastically increased the speed, size, and capability of computers when comparing the capacity of the mechanical computers created before Turing developed his theoretical model.

The rise of the World Wide Web signifies a major change in how people worldwide are able to connect with one another and exchange data. Quantum Computing created a new method of thinking about computing - one based on the concepts of Quantum Mechanics.

Technology drives the advancement of computing; however, the ethical aspects and social responsibility are equally important. The concepts of automation and greater use of machines, foreseen by early computer theorists such as Norbert Wiener, have become contemporary issues we deal with concerning artificial intelligence, privacy, and the impact of digital technology on our society. The history and development of computing, which has been extensively documented within these three documents, reflects a history of ongoing interaction among the different areas of science, technology, mathematics, and social systems that contributed to the growth of computing's success over time.

## Historical development and timeline of key events
(1821)- Charles Babbage first announced the invention of the difference engine in his first calculating paper on 14 June 1822, with about 2000 parts the difference engine number 1 was built by his engineer and master toolmaker Joseph Clement and completed in 1832. This finished portion of the unfinished engine is only one seventh of the complete design. Difference engine Num 2 between 1847 and 1849 Babbage designed a new engine which benefitted from the techniques developed for the more demanding analytical engine, the design was more efficient, requiring one third the number of parts of the first difference engine. 

In (1936)- computer science and machine learning can be traced back to ideas and early protocols such as Babbage's calculating engine in the early 19th century or Hollerith's punch card system from the end of the 19th century. There is however a strong case that Alan Turing machine laid the foundation for the development of computer science and machine learning in 1936. Alan Turing invented a mathematical model of a universal machine, which later became known as a Turing machine despite how simple it was, the Turing machine could be constructed to solve and given computer algorithm. This was the first concept of a universal all-purpose computing machine. It provided computer science with a firm scientific foundation, since it offered a model of computation which could be tested against real world applications.

(1945)- in 1942 john Mauchly proposed an all-electronic calculating machine, the result was ENIAC (electronic numerical integrator and computer) built between 1943 and 1945 the first large scale computer to run at electronic speed without being slowed by any mechanical parts. For a decade until a 1955 lightning stroked the ENIAC may have run more calculations that all mankind had done up to that point, much less well known are the six women who programmed ENIAC and truly brought it to life these women are Betty Holberton, Jean Jennings Bartik, kay McNulty, Marlyn Wescoff Meltzer, Ruth Lichterman and Frances Bilas Spencer. 

(1947)- The digital revolution began in 1947 with the invention of the solid-state transistor. John Bardeen, Walter Brattain and William Shockley, working for bell labs in the USA developed the revolutionary semiconductor device which can act as a switch turning tiny electric currents on or off, and as an amplifier of electric currents this could perform the same functions as vacuum tubes but was much smaller, more reliable and energy efficient, the impact of the transistor was immediate and far reaching it allowed for the creation of smaller more reliable electronic devices(the transistor radio). 

(1957)- integrated circuit the idea of the integrated circuit came around in about 1957 when the idea of creating small, ceramic wafers that each contained one component was first proposed by jack Kilby who worked for the us army his idea led to the micromodule program which held quite a bit of promise however as the project gained traction Kilby came up with another more advanced design that became the Integrated Circuit(IC) on September 12th 1958 Kilby demonstrated the first working IC and applied for a patent on February 6th 1959 Kilby's description of the device being a work of an electronic circuit was, for his part in inventing the IC Kilby won the Nobel prize in 2000. 

(1989)- The world wide web also known as WWW has reshaped human communication and made it possible to access vast amounts of information from any connected device from anywhere in the world the world wide web began in 1989 when British computer scientist sir Tim Berners less proposed a system to share information across different computers in 1990 Tim Berners lee received permission to develop the web further creating foundational technologies like html (Hypertext Markup Language), HTTP(Hypertext Transfer Protocol) and URL (Uniform Resource Locator) the web has revolutionized our daily lives, in terms of how we interact, learn, conduct business and engage with society this also improved and revolutionized communication with email, social media and instant messaging making getting a message to someone nearly instantaneously compared to what it would’ve been years ago during the world wars and even going back to the 80s that just shows how within a couple years how much humans have advanced. 

(2019)- Quantum supremacy in 2019 google claimed to have achieved quantum supremacy demonstrating that a quantum computer could perform a specific task faster than the most powerful classical computers this milestone highlighted the power of quantum computing for solving specific types of problems, quantum error correction advancements researchers  have made significant progress in quantum error correction developing new methods for detecting and fixing errors in quantum systems that is a vital step toward creating fault tolerance quantum computers the future of quantum computing research the future of quantum computing is very promising because as it is improving researchers are focusing on developing quantum software and practical applications, from drug discovery to financial modelling and optimization problems.  

## Scientific perspective
This study looks and views the scientific principles around each advancement, as computing has always relied on science, with mathematics for algorithms, physics for the electrical components and the information theory for communication and data structure.

In 1821, Charles Babbage’s design was based on finite differences, a mathematical method for calculating functions using repeated addition. This removed the need for multiplication and division, a very useful breakthrough in the field of science at the time. His machine was an attempt to apply pure mathematical theory to practical tasks, but the scientific limitations of the time help back his ideas, since the mechanical precision was not good enough for the mathematical requirements and the materials of the early 19th century brought issues regarding friction and durability. Even unfinished, the Difference Machine proved that mathematics could be mechanized, the fundamental idea behind computer science.

In 1936, Turing created the Universal Machine. Turing created an abstract scientific model describing how any computation could be carried out using symbolic rules. Turing’s model introduced three scientific ideas, algorithms as formal, testable processes, computability theory, which is what can or can’t be computed and the universal machine, which could simulate every other machine. Turing provided a scientific foundation for modern computing before physical computers existed, as he effectively separated science from engineering, and his theory still underpins all modern programming.

In 1945, The ENIAC was built using scientific knowledge of electrical switching and vacuum tubes. It showed how electrical circuits could implement the principles that Turing had described in his model. The ENIAC demonstrated large scale numerical calculations and validated electronic computation as a reliable scientific 
method. The scientific success of the ENIAC helped establish computing as a legitimate field of science rather than just an engineering experiment.

In 1947, the Transistor relied on quantum physics and the understanding of electron movements while in semiconductors. This advancement solved some key scientific problems found in vacuum tubes, like the heat generation and power consumption. The Transistors provided faster and much more stable switching, making them ideal for implementing Boolean logic electronically, and they marked a scientific shift from bulky and unreliable electronics to precise solid state physics.

The creation of integrated circuits in 1957 added to the theory of miniaturization science. Integrated circuits where able to connect multiple transistors on a single piece of silicon using insights from solid state physics and micro scale electrical behavior. integrated circuits solved a few scientific challenges such as resistance issues and heat microscopic levels. The development of integrated circuits paved the way towards Moore’s law, a scientific observation about exponential growth in transistor density

The big invention of the century was the World Wide Web, which had been seen as a software or networking invention, when in actuality it was grounded in scientific ideas about information organization. HTML provided a structured and logical way of representing data and URLs created a standardized addressing system based on scientific principles of referencing. The web demonstrated that information and data could be formalized using predictable and testable rules, a scientific breakthrough that shaped the future of computer science forever.

In 2019, Quantum Computing became a popular practice, as it relies entirely on quantum mechanics. Google did a lot of work in this field, with googles quantum supremacy result demonstrating that quantum states can solve certain problems vastly faster than the classical machines, but we have yet to get quantum computing to work perfectly for us, as some of the challenges we faced make it so that we cannot further studies on it, like decoherence, where qubits lose their quantum states and error correction methods to stabilize the quantum systems. This research represents computing returning to pure science, where the laws of physics dictate what is possible.

This scientific viewpoint shows that computing is like any other scientific field, with a continuous chain of discoveries rather than singled out inventions, with mathematics, physics and information theory being the real foundational drivers behind computing evolution. Throughout history science has not just backed up scientific discoveries of computing, but defined what computers could become at every stage.

## Mathematical Perspective
Computers were not invented as fast as overnight. Computers were made slowly as people were trying to make them using mathematics and engineering to build ways to solve problems.

(1821-1832) Charles Babbage and the difference engine Charles Babbage had made the Difference machine to fix mistakes in mathematical ways. The machine used a method of mathematics called finite differences (basically lets you calculate values using repeated addition). This made the machine easier to make and more dependable than human calculation. A simple working part with around 2,000 pieces had been built in 1832 and had proved to us that mechanical mathematics was possible.

(1936) Alan Turing and the Turing Machine Alan Turing announced the Turing Machine, a mathematical model which shows that any algorithm can indeed be carried out step by step. This idea of his demonstrated what computers can and cannot calculate. This became the base of computer science and really helped shape modern computing and AI.
(1945) ENIAC First Large Electronic Computer ENIAC was the first large electronic computer. It resolved complex mathematics problems such as differential equations for artillery. Considering it used vacuum tubes and not mechanical parts, it could calculate much faster than older machines. Six women programmers played a key part in making it all work.

(1947) The Transistor The creation of the transistor in 1947 started the digital age. What a transistor is; it basically acts like a tiny switch for electrical signals. It made binary logic (0s and 1s) possible in small fast machines. This replaced vacuum tubes and guided to modern electronics.
(1989) The World Wide Web (WWW) Tim Berners-Lee created the World Wide Web to make information easier to share across all computers. He had invented HTTP, HTML and URLs. The web changed everything like communication, learning, business, and daily life by making information available anywhere at all.
(2019) Quantum Supremacy In 2019, Google showed us that a quantum computer can solve a specific problem faster than any normal computer. That's called quantum supremacy. Quantum computing uses physics instead of the normal binary mathematics. It may help with future problems like medicine, science, and finance.


### Ethical reflections and improvement
As computing progresses with ages, ethical concerns emerged as part of the challenges faced in the computing industry. In the mathematics and in the scientific world there has been a lot of ethical challenges faced by stake holders in embracing technology and they have impacted the industries negatively. The early ethic challenges were seen as early as the 1950s by Wiener N, who was most concerned that people will tend to rely too much on machines and that with time the ,machines will exceed human ability. Walter M, (1970) also urged that computer technology created unique ethical problems that require new moral principles and policies. The three main challenges that have been faced in the history of computing are algorithmic bias and fairness, accountability and transparency, digital divide and Access inequality.

### Algorithmic Bias and Fairness:
Artificial intelligence models and algorithms are fundamental in science and in mathematics. These are developed by people and trained through existing patterns. These may be biased and produce biased results which end up affecting the stakeholders. An example is the hiring system that most companies where artificial intelligence is used to sample out the best fit candidate for the job from scanning through their curriculum vitae’s. As this system has been a good tool for companies, it has however proved of its inaccuracy and some people most deserving of the job get rejected due to their presentation on their curriculum vitae. It the becomes unfair to the industry and the jobseekers. Another example is the bank loan approvals system which follows the pattern of what was fed into it and therefore disadvantage other parts of the society.  

### Accountability and transparency
Some of the advanced artificial intelligence technologies are difficult to understand and interpret. This creates uncertainty and mistrust since there is no one to take responsibility. The explanations or whatsoever in terms of how an algorithm decisions were made hence it is difficult to fix the errors, bias and to also ensure fairness  Hence if anything is to go wrong, there is no one to be held accountable for the outcomes of it. The automatic systems can make a mistake or cause harm for example in the healthcare industry if a patient is mis-diagnosed there can be fatal outcomes, and there is a controversy as to who is to be blamed whether the developers or the users or the system, this then becomes an ethical legal issue.

### Digital divide and Access inequality
The progression of technology in science and mathematics solely aims at improving data driven insights in healthcare, education and finance, however these resources are not available to all the stakeholders. There is a notable gap between the people in the technical industry and other society members. This causes socio-economic discrimination as the few people who are informed technologically tend to benefit more than the others. As of Ireland’s 2026 Budget there is €618 million is allocated to in the Department of Enterprise, to establish new offices and other technological infrastructure. Technology has become a necessity, and governments are investing more into it than other sectors not only in Ireland but all over Europe and other continents too. The new technologies that are introduced are getting more advanced and complicated making it difficult for people who are not in the industry to understand and they end up not benefiting from them. Environmentally, it has been voiced out that the new technological machines that are used in mining are harmful to the environment.
 
## Future improvements
Since technology is part of our societies at present, despite its cons it has helped improve daily lives at large. A few strategies have been put in place since the early 50s to try and contain technologies in an ethical manner. There have been improvements in terms of rules and codes of conducts, however with the way the technological industry is growing there is need for more restrictions. 2021 et.al postulates that “The technology industry is also becoming more introspective, examining its own ethical principles, and exploring how to better manage its size and authority”. To address issues that have been seen tech industry players establish advisory panels, guiding principles, and the sponsoring of academic programs.

Lack of accountability can be solved if all the stake holders are bound to be accountable for the use of Artifitial Intelligence and Generative AI. Stake holders also need to be aware and avoid relying too much on Artificial intelligence to avoid biases. In the education sector, student should avoid using the Artificial Intelligence outputs wihtou their own added information and have it help in reaserch rather than to do the whole report. Environmentally, people have to be educated on disposing the machines. Improvement in energy efficient devices and promte the use of recycled material where relevent. There  there is currently a push in of tech companies to be more involved in energy re-usage and to sustain the environment. Companies like Amazon, HubSpot, Google just to mention are already striving to to proce environmentally friendly gandgets cars and so forth and so on.

Lastly Digital devide can be spparoached in such a way that the available resources are shared within the states equally to the industries.From educational scholarships to the governemnt bugdet the opportunities should be shared fairly between all departments. There should be codes of conducts put in place that ristrict people who are in tech from advancing in technology and benefiting individuals or just the tech department, but to be distribiuted to everyone equally no matter which industry they work on.  

## Conclusion

It is evident from a combined reading of all three documents that the evolution of computing technology should not be thought of simply as an unbroken chain of separate inventions, but rather as an ongoing process, where computing technologies have crystalized from a collective synergy of mathematics, science and society. In addition, through this historical evolution, we see that concepts that were initially not thought of as physically implementable, namely mechanical calculators, Boolean algebra, formal algorithms, calculation models—have successively been realized in tangible form as machines or highly sophisticated digital systems capable of performing billions of calculations per second. This transition from abstract thought to functional machine was only possible because of mathematics (i.e., logical framework) and science (i.e., tools and knowledge) enabling the transition of a concept to a functional system.

From a scientific standpoint, each major breakthrough relied not only on engineering but also on fundamental theories. The transistor emerged from quantum physics; integrated circuits required knowledge of microelectronics; digital communication is rooted in information theory; and quantum computing depends entirely on the study of qubits, superposition, and quantum behavior. The mathematical perspective reinforces this understanding by showing that every component of modern computing—from basic algorithms to global networking systems—is grounded in areas such as algebra, functions, graph theory, probability, computational geometry, and logical structures.

At the same time, the ethical reflections presented in the documents show that technological growth brings both opportunities and challenges. Wiener noted that the concern about automation in employment and our dependency on machines is not a new concern. Since the creation of digital systems that are based on math and computers, there is also growing concern with issues relating to disinformation, protecting individual privacy, and upholding social responsibilities associated with these digital platforms. These issues will require us to put not just technical innovation but also a good understanding of the ethical implications of technology and its governance to develop a system that works to benefit society as a whole.

In essence, the three documents demonstrate that computing is a dynamic and continually evolving field shaped by mathematical ideas, scientific discoveries, and societal needs. Its past highlights human ingenuity, its present reflects its global importance, and its future remains open calling for a balanced approach that integrates innovation, ethics, and knowledge to guide the next generations of technological development.


## References:


Alan Turing. (1936). Computer Science – Evolution of computers in society. https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&as_vis=1&q=Alan+Turing.+%281936%29.+Computer+Science+%E2%80%93+Evolution+of+computers+in+society.+&btnG=

Bardeen, J., Brattain, W., Shockley, W., & Bell Labs. (n.d.). Computer Science – Evolution of Computers in Society. In Early Computers and Computing Technology.
 
Charles Babbage. https://www.sciencemuseum.org.uk/objects-and-stories/charles-babbages-difference-engines-and-science-museum 

Connolly, C. (2025, May 21). Understanding the World Wide Web in 2024: Evolution and future. ProfileTree Web Design and Digital Marketing. 

ENIAC https://www.computerhistory.org/revolution/birth-of-the-computer/4/78 

https://arrow.tudublin.ie/cgi/viewcontent.cgi?article=1014&context=scschcomrep

https://curriculumonline.ie/getmedia/718bd569-5822-4795-b606-fc659c7159fb/NCCA-The-
Evolution-of-Computers-in-Society-LC-SC-solid-state.pdf 

https://profiletree.com/what-is-the-world-wide-web/ 

https://syntero.webflow.io/blog/the-evolution-of-transistors-from-vacuum-tubes-to-modern-semiconductors 

https://www.eniac.vc/writings/remembering-the-eniac-six Transistors 

https://www.spinquanta.com/news-detail/quantum-computing-research-pioneering-the-future-of-tech202501161051 

Integrated circuit (IC).https://anysilicon.com/history-integrated-circuit/ 
Norbert Wiener The Human Use of Human Beings (1950).https://www.britannica.com/science/control-theory-mathematics/Control-of-large-systems

Paul H Silvergate 2021 https://www.deloitte.com/us/en/insights/industry/technology/ethical-dilemmas-in-technology.html

Quantum supremacy SpinQ Quantum Computer | Quantum Computing Company | SpinQ technology. (n.d.). https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&as_vis=1&q=Quantum+supremacy+SpinQ+Quantum+Computer+%7C+Quantum+Computing+Company+%7C+SpinQ+technology.+%28n.d.%29.&btnG=

The evolution of transistors: from vacuum tubes to modern semiconductors | Syntero blog. (n.d.). https://pubs.aip.org/avs/jvb/article-abstract/30/6/060801/103375/Historical-development-and-future-trends-of-vacuum?redirectedFrom=fulltext

Walter M, (1970)https://scholar.google.com/scholar?q=Walter+M,+(1970)&hl=en&as_sdt=0&as_vis=1&oi=scholart
 
https://curriculumonline.ie/getmedia/96dfcf0a-8373-48d3-9662-714ab8c8a428/NCCA-The-Evolution-of-Computers-in-Society-LC-SC-the-turing-machine.pdf?utm_source=chatgpt.com 

https://www.britannica.com/technology/ENIAC

https://www.electronics-tutorials.ws/logic/logic-gates-using-transistors.html

https://www.coursehero.com/study-guides/zeliite115/reading-random-access-memory

https://www.parc.com/innovations/gui/

https://www.britannica.com/technology/3D-printing

https://www.internetsociety.org/internet/history-of-the-internet



