# this is done by milon !
## this was edited by Andile

## Historical development

## Introduction 

The introduction of computers brought civilization to the whole world with so much to offer to other industries. From the beginning of the first machine that was invented to date there had been a notable milestone improvement. The report traces the timeline of machine innovation from the beginning to present day. The report was discussed from a scientific and mathematical viewpoint which also traces the history of the machines in the specific sectors. The ethic reflections were discussed explaining the challenges that have been faced and the future improvements already in place to avoid the ethical challenges note with relevant examples were given.  

## Historical development and timeline of key events

(1821)- Charles Babbage first announced the invention of the difference engine in his first calculating paper on 14 June 1822, with about 2000 parts the difference engine number 1 was built by his engineer and master toolmaker Joseph Clement and completed in 1832. This finished portion of the unfinished engine is only one seventh of the complete design. Difference engine Num 2 between 1847 and 1849 Babbage designed a new engine which benefitted from the techniques developed for the more demanding analytical engine, the design was more efficient, requiring one third the number of parts of the first difference engine. 

In (1936)- computer science and machine learning can be traced back to ideas and early protocols such as Babbage's calculating engine in the early 19th century or Hollerith's punch card system from the end of the 19th century. There is however a strong case that Alan Turing machine laid the foundation for the development of computer science and machine learning in 1936. Alan Turing invented a mathematical model of a universal machine, which later became known as a Turing machine despite how simple it was, the Turing machine could be constructed to solve and given computer algorithm. This was the first concept of a universal all-purpose computing machine. It provided computer science with a firm scientific foundation, since it offered a model of computation which could be tested against real world applications.

(1945)- in 1942 john Mauchly proposed an all-electronic calculating machine, the result was ENIAC (electronic numerical integrator and computer) built between 1943 and 1945 the first large scale computer to run at electronic speed without being slowed by any mechanical parts. For a decade until a 1955 lightning stroked the ENIAC may have run more calculations that all mankind had done up to that point, much less well known are the six women who programmed ENIAC and truly brought it to life these women are Betty Holberton, Jean Jennings Bartik, kay McNulty, Marlyn Wescoff Meltzer, Ruth Lichterman and Frances Bilas Spencer. 

(1947)- The digital revolution began in 1947 with the invention of the solid-state transistor. John Bardeen, Walter Brattain and William Shockley, working for bell labs in the USA developed the revolutionary semiconductor device which can act as a switch turning tiny electric currents on or off, and as an amplifier of electric currents this could perform the same functions as vacuum tubes but was much smaller, more reliable and energy efficient, the impact of the transistor was immediate and far reaching it allowed for the creation of smaller more reliable electronic devices(the transistor radio). 

(1957)- integrated circuit the idea of the integrated circuit came around in about 1957 when the idea of creating small, ceramic wafers that each contained one component was first proposed by jack Kilby who worked for the us army his idea led to the micromodule program which held quite a bit of promise however as the project gained traction Kilby came up with another more advanced design that became the Integrated Circuit(IC) on September 12th 1958 Kilby demonstrated the first working IC and applied for a patent on February 6th 1959 Kilby's description of the device being a work of an electronic circuit was, for his part in inventing the IC Kilby won the Nobel prize in 2000. 

(1989)- The world wide web also known as WWW has reshaped human communication and made it possible to access vast amounts of information from any connected device from anywhere in the world the world wide web began in 1989 when British computer scientist sir Tim Berners less proposed a system to share information across different computers in 1990 Tim Berners lee received permission to develop the web further creating foundational technologies like html (Hypertext Markup Language), HTTP(Hypertext Transfer Protocol) and URL (Uniform Resource Locator) the web has revolutionized our daily lives, in terms of how we interact, learn, conduct business and engage with society this also improved and revolutionized communication with email, social media and instant messaging making getting a message to someone nearly instantaneously compared to what it would’ve been years ago during the world wars and even going back to the 80s that just shows how within a couple years how much humans have advanced. 

(2019)- Quantum supremacy in 2019 google claimed to have achieved quantum supremacy demonstrating that a quantum computer could perform a specific task faster than the most powerful classical computers this milestone highlighted the power of quantum computing for solving specific types of problems, quantum error correction advancements researchers  have made significant progress in quantum error correction developing new methods for detecting and fixing errors in quantum systems that is a vital step toward creating fault tolerance quantum computers the future of quantum computing research the future of quantum computing is very promising because as it is improving researchers are focusing on developing quantum software and practical applications, from drug discovery to financial modelling and optimization problems.  

## Scientific perspective

This study looks and views the scientific principles around each advancement, as computing has always relied on science, with mathematics for algorithms, physics for the electrical components and the information theory for communication and data structure.

In 1821, Charles Babbage’s design was based on finite differences, a mathematical method for calculating functions using repeated addition. This removed the need for multiplication and division, a very useful breakthrough in the field of science at the time. His machine was an attempt to apply pure mathematical theory to practical tasks, but the scientific limitations of the time help back his ideas, since the mechanical precision was not good enough for the mathematical requirements and the materials of the early 19th century brought issues regarding friction and durability. Even unfinished, the Difference Machine proved that mathematics could be mechanized, the fundamental idea behind computer science.

In 1936, Turing created the Universal Machine. Turing created an abstract scientific model describing how any computation could be carried out using symbolic rules. Turing’s model introduced three scientific ideas, algorithms as formal, testable processes, computability theory, which is what can or can’t be computed and the universal machine, which could simulate every other machine. Turing provided a scientific foundation for modern computing before physical computers existed, as he effectively separated science from engineering, and his theory still underpins all modern programming.

In 1945, The ENIAC was built using scientific knowledge of electrical switching and vacuum tubes. It showed how electrical circuits could implement the principles that Turing had described in his model. The ENIAC demonstrated large scale numerical calculations and validated electronic computation as a reliable scientific 
method. The scientific success of the ENIAC helped establish computing as a legitimate field of science rather than just an engineering experiment.

In 1947, the Transistor relied on quantum physics and the understanding of electron movements while in semiconductors. This advancement solved some key scientific problems found in vacuum tubes, like the heat generation and power consumption. The Transistors provided faster and much more stable switching, making them ideal for implementing Boolean logic electronically, and they marked a scientific shift from bulky and unreliable electronics to precise solid state physics.

The creation of integrated circuits in 1957 added to the theory of miniaturization science. Integrated circuits where able to connect multiple transistors on a single piece of silicon using insights from solid state physics and micro scale electrical behavior. integrated circuits solved a few scientific challenges such as resistance issues and heat microscopic levels. The development of integrated circuits paved the way towards Moore’s law, a scientific observation about exponential growth in transistor density

The big invention of the century was the World Wide Web, which had been seen as a software or networking invention, when in actuality it was grounded in scientific ideas about information organization. HTML provided a structured and logical way of representing data and URLs created a standardized addressing system based on scientific principles of referencing. The web demonstrated that information and data could be formalized using predictable and testable rules, a scientific breakthrough that shaped the future of computer science forever.

In 2019, Quantum Computing became a popular practice, as it relies entirely on quantum mechanics. Google did a lot of work in this field, with googles quantum supremacy result demonstrating that quantum states can solve certain problems vastly faster than the classical machines, but we have yet to get quantum computing to work perfectly for us, as some of the challenges we faced make it so that we cannot further studies on it, like decoherence, where qubits lose their quantum states and error correction methods to stabilize the quantum systems. This research represents computing returning to pure science, where the laws of physics dictate what is possible.

This scientific viewpoint shows that computing is like any other scientific field, with a continuous chain of discoveries rather than singled out inventions, with mathematics, physics and information theory being the real foundational drivers behind computing evolution. Throughout history science has not just backed up scientific discoveries of computing, but defined what computers could become at every stage.

## Mathematical Perspective
Computers were not invented as fast as overnight. Computers were made slowly as people were trying to make them using mathematics and engineering to build ways to solve problems.

(1821-1832) Charles Babbage and the difference engine Charles Babbage had made the Difference machine to fix mistakes in mathematical ways. The machine used a method of mathematics called finite differences (basically lets you calculate values using repeated addition). This made the machine easier to make and more dependable than human calculation. A simple working part with around 2,000 pieces had been built in 1832 and had proved to us that mechanical mathematics was possible.

(1936) Alan Turing and the Turing Machine Alan Turing announced the Turing Machine, a mathematical model which shows that any algorithm can indeed be carried out step by step. This idea of his demonstrated what computers can and cannot calculate. This became the base of computer science and really helped shape modern computing and AI.
(1945) ENIAC First Large Electronic Computer ENIAC was the first large electronic computer. It resolved complex mathematics problems such as differential equations for artillery. Considering it used vacuum tubes and not mechanical parts, it could calculate much faster than older machines. Six women programmers played a key part in making it all work.

(1947) The Transistor The creation of the transistor in 1947 started the digital age. What a transistor is; it basically acts like a tiny switch for electrical signals. It made binary logic (0s and 1s) possible in small fast machines. This replaced vacuum tubes and guided to modern electronics.
(1989) The World Wide Web (WWW) Tim Berners-Lee created the World Wide Web to make information easier to share across all computers. He had invented HTTP, HTML and URLs. The web changed everything like communication, learning, business, and daily life by making information available anywhere at all.
(2019) Quantum Supremacy In 2019, Google showed us that a quantum computer can solve a specific problem faster than any normal computer. That's called quantum supremacy. Quantum computing uses physics instead of the normal binary mathematics. It may help with future problems like medicine, science, and finance.


## Ethical reflections and improvement
As computing progresses with ages, ethical concerns emerged as part of the challenges faced in the computing industry. In the mathematics and in the scientific world there has been a lot of ethical challenges faced by stake holders in embracing technology and they have impacted the industries negatively. The early ethic challenges were seen as early as the 1950s by Wiener N, who was most concerned that people will tend to rely too much on machines and that with time the ,machines will exceed huma ability. Walter M, (1970) also urged that computer technology created unique ethical problems that require new moral principles and policies. The three main challenges that have been faced in the history of computing are algorithmic bias and fairness, accountability and transparency, digital divide and Access inequality.

## Algorithmic Bias and Fairness:
Artificial intelligence models and algorithms are fundamental in science and in mathematics. These are developed by people and trained through existing patterns. These may be biased and produce biased results which end up affecting the stakeholders. An example is the hiring system that most companies where artificial intelligence is used to sample out the best fit candidate for the job from scanning through their curriculum vitae’s. As this system has been a good tool for companies, it has however proved of its inaccuracy and some people most deserving of the job get rejected due to their presentation on their curriculum vitae. It the becomes unfair to the industry and the jobseekers. Another example is the bank loan approvals system which follows the pattern of what was fed into it and therefore disadvantage other parts of the society.  

## Accountability and transparency

Some of the advanced artificial intelligence technologies are difficult to understand and interpret. This creates uncertainty and mistrust since there is no one to take responsibility. The explanations or whatsoever in terms of how an algorithm decisions were made hence it is difficult to fix the errors, bias and to also ensure fairness  Hence if anything is to go wrong, there is no one to be held accountable for the outcomes of it. The automatic systems can make a mistake or cause harm for example in the healthcare industry if a patient is mis-diagnosed there can be fatal outcomes, and there is a controversy as to who is to be blamed whether the developers or the users or the system, this then becomes an ethical legal issue.

#Digital divide and Access inequality
The progression of technology in science and mathematics solely aims at improving data driven insights in healthcare, education and finance, however these resources are not available to all the stakeholders. There is a notable gap between the people in the technical industry and other society members. This causes socio-economic discrimination as the few people who are informed technologically tend to benefit more than the others. As of Ireland’s 2026 Budget there is €618 million is allocated to in the Department of Enterprise, to establish new offices and other technological infrastructure. Technology has become a necessity, and governments are investing more into it than other sectors not only in Ireland but all over Europe and other continents too. The new technologies that are introduced are getting more advanced and complicated making it difficult for people who are not in the industry to understand and they end up not benefiting from them. Environmentally, it has been voiced out that the new technological machines that are used in mining are harmful to the environment.
 
## Future improvements
Since technology is part of our societies at present, despite its cons it has helped improve daily lives at large. A few strategies have been put in place since the early 50s to try and contain technologies in an ethical manner. There have been improvements in terms of rules and codes of conducts, however with the way the technological industry is growing there is need for more restrictions. 2021 et.al postulates that “The technology industry is also becoming more introspective, examining its own ethical principles, and exploring how to better manage its size and authority”. This shows that the industry understands the lack of accountability in artificial intelligence and aims at improving each sector. In terms of the environment, there is currently a push in of tech companies to be more involved in energy re usage and to sustain the environment. Companies like Amazon, HubSpot, google just to mention a few have already embraced the idea. In artificial intelligence, increasingly important that AI-powered systems operate under principles that benefit society and avoid issues with bias, fairness, transparency, and explainability. To address issues that have been seen tech industry players establish advisory panels, guiding principles, and the sponsoring of academic programs.


## Conclusion
In conclusion, the noted computer innovations from the first that was innovated until present day. The discussion focused on two main viewpoints, the scientific and the mathematical. The timelines were explained step by step. Challenges that have merged with technology were noted and explained with examples where necessary. Lastly there are some future recommendations put in place to avoid the ethical challenges that are already in place. 

## References:
Norbert Wiener The Human Use of Human Beings (1950)
https://arrow.tudublin.ie/cgi/viewcontent.cgi?article=1014&context=scschcomrep
 Paul H Silvergate 2021 https://www.deloitte.com/us/en/insights/industry/technology/ethical-dilemmas-in-technology.html
Charles Babbage engine 

https://www.sciencemuseum.org.uk/objects-and-stories/charles-babbages-difference-engines-and-science-museum 

ENIAC 

https://www.computerhistory.org/revolution/birth-of-the-computer/4/78 

https://www.eniac.vc/writings/remembering-the-eniac-six 

Transistors 
Bardeen, J., Brattain, W., Shockley, W., & Bell Labs. (n.d.). Computer Science – Evolution of Computers in Society. In Early Computers and Computing Technology. 

https://curriculumonline.ie/getmedia/718bd569-5822-4795-b606-fc659c7159fb/NCCA-The-Evolution-of-Computers-in-Society-LC-SC-solid-state.pdf 

The evolution of transistors: from vacuum tubes to modern semiconductors | Syntero blog. (n.d.). 

https://syntero.webflow.io/blog/the-evolution-of-transistors-from-vacuum-tubes-to-modern-semiconductors 

Integrated circuit (IC)  

https://anysilicon.com/history-integrated-circuit/ 

Quantum Supremacy 

Quantum supremacy SpinQ Quantum Computer | Quantum Computing Company | SpinQ technology. (n.d.). 

https://www.spinquanta.com/news-detail/quantum-computing-research-pioneering-the-future-of-tech202501161051 

World wide web 

Connolly, C. (2025, May 21). Understanding the World Wide Web in 2024: Evolution and future. ProfileTree Web Design and Digital Marketing. https://profiletree.com/what-is-the-world-wide-web/ 

Alan Turing. (1936). Computer Science – Evolution of computers in society. https://curriculumonline.ie/getmedia/96dfcf0a-8373-48d3-9662-714ab8c8a428/NCCA-The-Evolution-of-Computers-in-Society-LC-SC-the-turing-machine.pdf?utm_source=chatgpt.com 


