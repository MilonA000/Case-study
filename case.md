# this is done by milon !
## this was edited by Andile

# Historical development

(1821)- Charles Babbage first announced the invention of the difference engine in his first calculating paper on 14 June 1822, with about 2000 parts the difference engine number 1 was built by his engineer and master toolmaker joseph clement and completed in 1832 this finished portion of the unfinished engine is only one seventh of the complete design. Difference engine Num 2 between 1847 and 1849 Babbage designed a new engine which benefitted from the techniques developed for the more demanding analytical engine the design was more efficient, requiring one third the number of parts of the first difference engine. 

(1936)- computer science and machine learning can be traced back to ideas and early protocols such as Babbage's calculating engine in the early 19th century or Hollerith's punch card system from the end of the 19th century there is however a strong case that Alan Turings machine laid the foundation for the development of computer science and machine learning in 1936 Alan Turing invented a mathematical model of a universal machine, which later became known as a Turing machine despite how simple it was the Turing machine can be constructed to solve and given computer algorithm in this wat this was the first concept of a universal all-purpose computing machine it provide computer science with a firm  scientific foundation, since it offers a model of computation which can be tested against real world applications.

(1945)- in 1942 john Mauchly proposed an all-electronic calculating machine, the result was ENIAC (electronic numerical integrator and computer) built between 1943 and 1945 the first large scale computer to run at electronic speed without being slowed by any mechanical parts. For a decade until a 1955 lightning strike ENIAC may have run more calculations that all mankind had done up to that point, much less well known are the six women who programmed ENIAC and truly brought it to life these women are Betty Holberton, Jean Jennings Bartik, kay McNulty ,Marlyn Wescoff Meltzer, Ruth Lichterman and Frances Bilas Spencer. 

(1947)- The digital revolution began in 1947 with the invention of the solid-state transistor. John Bardeen, Walter Brattain and William Shockley, working for bell labs in the USA developed the revolutionary semiconductor device which can act as a switch turning tiny electric currents on or off, and as an amplifier of electric currents this could perform the same functions as vacuum tubes but was much smaller, more reliable and energy efficient, the impact of the transistor was immediate and far reaching it allowed for the creation of smaller more reliable electronic devices(the transistor radio). 

(1957)- integrated circuit the idea of the integrated circuit came around in about 1957 when the idea of creating small, ceramic wafers that each contained one component was first proposed by jack Kilby who worked for the us army his idea led to the micromodule program which held quite a bit of promise however as the project gained traction Kilby came up with another more advanced design that became the Integrated Circuit(IC) on September 12th 1958 Kilby demonstrated the first working IC and applied for a patent on February 6th 1959 Kilby's description of the device being a work of an electronic circuit was, for his part in inventing the IC Kilby won the Nobel prize in 2000. 

(1989)- The world wide web also known as WWW has reshaped human communication and made it possible to access vast amounts of information from any connected device from anywhere in the world the world wide web began in 1989 when British computer scientist sir Tim Berners less proposed a system to share information across different computers in 1990 Tim Berners lee received permission to develop the web further creating foundational technologies like html (Hypertext Markup Language), HTTP(Hypertext Transfer Protocol) and URL (Uniform Resource Locator) the web has revolutionised our daily lives, in terms of how we interact, learn, conduct business and engage with society this also improved and revolutionised communication with email,social media and instant messaging making getting a message to someone nearly instantaneously compared to what it wouldve been years ago during the world wars and even going back to the 80s that just shows how within a couple years how much humans have advanced. 

(2019)- Quantum supremacy in 2019 google claimed to have achieved quantum supremacy demonstrating that a quantum computer could perform a specific task faster than the most powerful classical computers this milestone highlighted the power of quantum computing for solving specific types of problems, quantum error correction advancements researchers  have made significant progress in quantum error correction developing new methods for detecting and fixing errors in quantum systems that is a vital step toward creating fault tolerance quantum computers the future of quantum computing research the future of quantum computing is very promising because as it is improving researchers are focusing on developing quantum software and practical applications, from drug discovery to financial modelling and optimization problems.  

# Viewpoint 1, scientific perspective

we will look at this study and view the scientific principles around each advancement, as computing has always relied on science, with mathmatics for algorithms, physics for the electrical components and the information theory for communication and data structure.

in 1821, Charles Babbages design was based on finite differences, a mathmatical method for calculating functions using repeated addition. This removed the need for multiplication and division, a very useful breakthrough in the field of science at the time. His machine was an attempt to apply pure mathmatical theory to practical tasks, but the scientific limitations of the time help back his ideas, since the mechanical precision was not good enough for the mathmatical requirments and the materials of the early 19th century brought issues regarding friction and durability. Even unfinished, the Difference Machine proved that mathmatics could be mechanised, the fundemental idea behind computer science.

In 1936, Turing created the Universal Machine. Turing created an abstract scientific model describing how any compution could be carried out using symbolic rules. Turings model introduced three sceintific ideas, algorithims as formal, testable procceses, computability theory, which is what can or cant be computed and the universal machine, which could simulate every other machine. Turing provied a scientific foundation for modern computing before physical computers existed, as he effectively seperated science from engeneering, and his theory still underpins all modern programming.

In 1945, The ENIAC was built using sceintific knowlege of electrical switching and vacume tubes. It showed how electrical circuits could implement the principles that Turing had described in his model. The ENIAC demonstrated large scale numerical calculations and validated electronic compution as a reliable scientific 
method. The scientific success of the ENIAC helped establish computing as a legitimate field of science rahter than just an engeneering experiment.

In 1947, the Transistor relied on quantem physics and the understanding of electron movements while in semiconducters. This advancement solved some key scientific probloms found in vacuum tubes, like the heat generation and power consumption. The Transistors provided faster and much more stable switching, making them ideal for implementing boolean logic electornically, and they marked a scientific shift from bulky and unreliable electronics to precise solid state physics.

The creation of integrated cicruits in 1957 added to the theory of miniturisation science. Integrated circuits where able to connect multiple transistors on a single peice of silicon using insights from solid state physics and micro scale electrical behavour. intergrated circuits solved a few scientific challenges such as resistance issues and heat microscopic levels. The development of intigrated circuits paved the way towards Moores law, a scientific observation about exponential growth in transistor density

The big invention of the century was the World Wide Web, which had been seen as a software or networking invention, when in actuallity it was grounded in scientific ideas about information organisation. HTML provided a structured and logical way of representing data and URLs created a standerdised addressing system based on scientific principles of refrencing. The web demonstrated that information and data could be formalised using predictable and testable rules, a scientific breakthrough that shaped the future of computer science forever.

In 2019, Quantum Computing became a popular practice, as it relies entirely on quantum mechanics. Google did a lot of work in this feild, with googles quantum supremacy result demonstrating that quantum states can solve ceirtain probloms vastly fasteer than the classical machines, but we have yet to get quantum computing to work perfectly for us, as some of the challenges we faced make it so that we cannot furthur studies on it, like decoherence, where quibits lose their quantum states and error correction methods to stabalize the quantum systems. This research represents computing returning to pure science, where the laws of physics dictate what is possible.

This scientific viepoint shows that computing is like any other scientific field, with a continous chain of discoveries rather than singled out inventions, with mathmatics, physics and information theory being the real foundational drivers behind computing evelution. Throughout history science has not just backed up scientific discoveries of computing, but defined what computers could become at every stage.

# Ethical reflections and improvement
As computing progresses with ages , ethical concerns emerged as part of the challenges faced in the computing industry.  The early ethic challenges were seen as early as the 1950s by  Norbert Wiener . He was most concerned that people will tend to rely too much on machines and at the end they would end up without jobs. Even at present this remains a concern as they As a philosopher he wrote about how human would later rely too much on machines and the problems that are likely to be caused by such. In his book he explored the impact of machines in the society as a whole. Wiener , N (1950) postulates that, “Our advancements in technology have created new opportunities along with new restrictions.” This further explains his ideas about how people would be out of jobs. also stated that if the machine are not properly supervised, they will end up exceeding the human ability. He states that there is a danger in trusting something that doesn’t think abstractly. That is the social responsibilities of the scientists and engineers. 


Walter Maner 1970 was the first to develop the first computer ethics course . He urged that computer technology created unique ethical problems that required new moral principles and policies. Deborah Johnson’s “Computer Ethics” published in 1985 was the first major textbook in the field (Johnson, 1985). On page one, she noted that computers “pose new versions of standard moral problems and moral dilemmas, exacerbating the old problems, and forcing us to apply ordinary moral norms in uncharted realms.” Computers are logically malleable in that they can be shaped and moulded to do any activity that can be characterized in terms of inputs, outputs and connecting logical operations… Because logic applies everywhere, the potential applications of computer technology appear limitless. The computer is the nearest thing we have to a universal tool. Indeed, the limits of computers are largely the limits of our own creativity”. Moor 1885

Social media
Free speech is a human right, and social media is its facilitator. Social media facilitates free speech, unfettered but for the policing of abuse, and censoring of posts which violate societal norms. Commentators are divided over whether the value of a free speech arena is compromised by the ease with which bots and trolls are able to manipulate the system (Leerssen, 2015). Despite criticism over fake news and the current advertising and influence scandals, the digital giants are wary of actions that may open them up to accusations of bias. Traditional media outlets are already clear on their responsibilities. Reuters aims for “independence, integrity and freedom from bias.” So far, the influence of social media has been overlooked by regulators and providers. A central question is to what extent existing media ethics is suitable for todays and tomorrow’s media that is immediate, interactive and “always on” – a journalism of amateurs and professionals. Most of the principles were developed over the past century, originating in the construction of professional, objective ethics for mass commercial newspapers in the late 19th century.
### Recomendations 
one would recomend that the plocieas that are already in place to be implemented and concquences
